# Model Explainability Using SHAP Analysis
# Global feature importance and contributions across ecoregions
# Corresponds to Fig. 7(a,b) in the manuscript

"""
OVERVIEW
This notebook performs SHAP (SHapley Additive exPlanations) analysis to 
interpret the Random Forest model predictions for forest change loss.

METHODOLOGY
1. Loads pre-computed SHAP values from model predictions
2. Creates two complementary visualizations:
   a) Beeswarm plot showing global feature importance and effects
   b) Line plots showing feature contributions across different ecoregions

TECHNICAL DETAILS
- Uses TreeSHAP for efficient computation with tree-based models
- Analyzes 14 key features including spectral, texture, and temporal metrics
- Computes regional variations in feature importance
- Visualizes nonlinear relationships between features and predictions
"""

# ============================================================================
# ENVIRONMENT SETUP
# ============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import pickle
import warnings
from matplotlib import cm
from matplotlib.colors import LinearSegmentedColormap
warnings.filterwarnings('ignore')

# Set publication style with larger fonts
plt.rcParams['font.family'] = 'Arial'
plt.rcParams['font.size'] = 14
plt.rcParams['axes.labelsize'] = 16
plt.rcParams['axes.titlesize'] = 18
plt.rcParams['xtick.labelsize'] = 14
plt.rcParams['ytick.labelsize'] = 14
plt.rcParams['legend.fontsize'] = 12

# ============================================================================
# DATA LOADING
# ============================================================================

def load_shap_data(shap_file_path):
    """
    Load pre-computed SHAP data from pickle file.
    
    Parameters:
    -----------
    shap_file_path : str
        Path to pickle file containing SHAP data
        
    Returns:
    --------
    shap_data : dict
        Dictionary containing SHAP values, feature names, and metadata
    """
    try:
        with open(shap_file_path, 'rb') as f:
            shap_data = pickle.load(f)
        
        print("✓ SHAP data loaded successfully")
        print(f"  Number of samples: {len(shap_data.get('X', []))}")
        print(f"  Number of features: {len(shap_data.get('feature_names', []))}")
        
        # Print model performance if available
        if 'performance' in shap_data:
            perf = shap_data['performance']
            print(f"  Model performance:")
            print(f"    Training R²: {perf.get('train_r2', 'N/A'):.4f}")
            print(f"    Testing R²: {perf.get('test_r2', 'N/A'):.4f}")
        
        return shap_data
    except Exception as e:
        print(f"✗ Error loading SHAP data: {e}")
        print("\nIf you don't have pre-computed SHAP data, you can:")
        print("1. Train a Random Forest model on your data")
        print("2. Compute SHAP values using the shap.TreeExplainer")
        print("3. Save the results using pickle.dump()")
        
        # Return None to trigger simulated data
        return None

def create_simulated_shap_data(n_samples=1000, n_features=14):
    """
    Create simulated SHAP data for demonstration purposes.
    
    Parameters:
    -----------
    n_samples : int
        Number of samples to simulate
    n_features : int
        Number of features to simulate
        
    Returns:
    --------
    shap_data : dict
        Simulated SHAP data dictionary
    """
    print("Creating simulated SHAP data for demonstration...")
    
    # Feature names (matching manuscript)
    feature_names = [
        'NBR', 'NDVI',
        'NBR_con_texture', 'NBR_cor_texture', 'NBR_ent_texture',
        'NDVI_con_texture', 'NDVI_cor_texture', 'NDVI_ent_texture',
        'NBR_rol_3y_temporal', 'NBR_rol_5y_temporal', 'NBR_vola_5y_temporal',
        'NDVI_rol_3y_temporal', 'NDVI_rol_5y_temporal', 'NDVI_vola_5y_temporal'
    ][:n_features]
    
    # Simulate feature values (normalized)
    np.random.seed(42)
    X = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=feature_names
    )
    
    # Simulate SHAP values with realistic patterns
    shap_values = np.zeros((n_samples, n_features))
    
    # Assign realistic importance patterns
    importance_weights = np.array([0.15, 0.12, 0.08, 0.07, 0.06, 
                                   0.07, 0.06, 0.05, 0.09, 0.08, 
                                   0.07, 0.05, 0.04, 0.03][:n_features])
    
    for i in range(n_features):
        # Create feature-specific pattern
        if i == 0:  # NBR - strong positive effect
            shap_values[:, i] = 0.5 * X.iloc[:, i] + 0.1 * np.random.randn(n_samples)
        elif i == 1:  # NDVI - moderate negative effect
            shap_values[:, i] = -0.3 * X.iloc[:, i] + 0.1 * np.random.randn(n_samples)
        else:
            shap_values[:, i] = importance_weights[i] * X.iloc[:, i] * np.random.choice([-1, 1]) + 0.05 * np.random.randn(n_samples)
    
    # Simulate ecoregion information
    ecoregions = np.random.choice(['I01', 'I02', 'I03', 'I04', 'I05', 
                                  'I07', 'I08', 'I10', 'I11', 'I12'], 
                                  size=n_samples)
    
    shap_data = {
        'X': X,
        'shap_values': shap_values,
        'feature_names': feature_names,
        'ecoregion_info': ecoregions,
        'performance': {
            'train_r2': 0.85,
            'test_r2': 0.82
        }
    }
    
    return shap_data

# ============================================================================
# VISUALIZATION A: GLOBAL FEATURE IMPORTANCE (BEESWARM PLOT)
# ============================================================================

def plot_shap_beeswarm(shap_data, figsize=(14, 8), save_path=None):
    """
    Create beeswarm plot showing global feature importance and effects.
    
    Parameters:
    -----------
    shap_data : dict
        Dictionary containing SHAP data
    figsize : tuple
        Figure size (width, height)
    save_path : str, optional
        Path to save the figure
        
    Returns:
    --------
    fig : matplotlib.figure.Figure
        Generated figure
    """
    import shap
    
    # Extract data
    X = shap_data['X']
    shap_values = shap_data['shap_values']
    feature_names = shap_data['feature_names']
    
    # Feature name mapping for display
    feature_display_names = {
        'NBR': 'NBR',
        'NDVI': 'NDVI',
        'NBR_con_texture': 'NBR_con',
        'NBR_cor_texture': 'NBR_cor',
        'NBR_ent_texture': 'NBR_ent',
        'NDVI_con_texture': 'NDVI_con',
        'NDVI_cor_texture': 'NDVI_cor',
        'NDVI_ent_texture': 'NDVI_ent',
        'NBR_rol_3y_temporal': 'NBR_rol_3y',
        'NBR_rol_5y_temporal': 'NBR_rol_5y',
        'NBR_vola_5y_temporal': 'NBR_vola_5y',
        'NDVI_rol_3y_temporal': 'NDVI_rol_3y',
        'NDVI_rol_5y_temporal': 'NDVI_rol_5y',
        'NDVI_vola_5y_temporal': 'NDVI_vola_5y'
    }
    
    display_names = [feature_display_names.get(name, name) for name in feature_names]
    
    # Create custom colormap (blue-white-red)
    colors = ['#004DA8', '#FFFFFF', '#E64C00']
    cmap = LinearSegmentedColormap.from_list('custom_cmap', colors, N=256)
    
    # Create figure
    fig, ax = plt.subplots(figsize=figsize)
    
    # Create beeswarm plot using shap library
    shap.summary_plot(
        shap_values,
        X,
        feature_names=display_names,
        plot_type="dot",
        show=False,
        max_display=len(feature_names),
        cmap=cmap,
        plot_size=None
    )
    
    # Customize the plot
    ax = plt.gca()
    ax.set_xlabel('SHAP Value (Impact on Model Output)', 
                 fontsize=16, fontweight='bold')
    ax.set_ylabel('Features', fontsize=16, fontweight='bold')
    
    # Add zero line
    ax.axvline(x=0, color='gray', linestyle='--', alpha=0.7, linewidth=1)
    
    # Add grid
    ax.grid(True, alpha=0.3, axis='x')
    
    # Adjust tick labels
    ax.tick_params(axis='both', which='major', labelsize=14)
    
    plt.tight_layout()
    
    # Save figure if path provided
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        fig.savefig(f"{save_path}_beeswarm.png", dpi=300, bbox_inches='tight')
        fig.savefig(f"{save_path}_beeswarm.pdf", dpi=300, bbox_inches='tight')
        print(f"✓ Beeswarm plot saved: {save_path}_beeswarm.png")
    
    return fig

# ============================================================================
# VISUALIZATION B: ECOREGION-SPECIFIC FEATURE CONTRIBUTIONS
# ============================================================================

def plot_ecoregion_shap_contributions(shap_data, figsize=(20, 12), 
                                     top_features=None, save_path=None):
    """
    Create line plots showing feature contributions across ecoregions.
    
    Parameters:
    -----------
    shap_data : dict
        Dictionary containing SHAP data
    figsize : tuple
        Figure size (width, height)
    top_features : int or None
        Number of top features to display (None for all)
    save_path : str, optional
        Path to save the figure
        
    Returns:
    --------
    fig : matplotlib.figure.Figure
        Generated figure
    """
    # Extract data
    shap_values = shap_data['shap_values']
    feature_names = shap_data['feature_names']
    ecoregions = shap_data['ecoregion_info']
    
    # Feature name mapping for display
    feature_display_names = {
        'NBR': 'NBR',
        'NDVI': 'NDVI',
        'NBR_con_texture': 'NBR_con',
        'NBR_cor_texture': 'NBR_cor',
        'NBR_ent_texture': 'NBR_ent',
        'NDVI_con_texture': 'NDVI_con',
        'NDVI_cor_texture': 'NDVI_cor',
        'NDVI_ent_texture': 'NDVI_ent',
        'NBR_rol_3y_temporal': 'NBR_rol_3y',
        'NBR_rol_5y_temporal': 'NBR_rol_5y',
        'NBR_vola_5y_temporal': 'NBR_vola_5y',
        'NDVI_rol_3y_temporal': 'NDVI_rol_3y',
        'NDVI_rol_5y_temporal': 'NDVI_rol_5y',
        'NDVI_vola_5y_temporal': 'NDVI_vola_5y'
    }
    
    # Calculate feature importance for sorting
    feature_importance = np.mean(np.abs(shap_values), axis=0)
    
    # Select features to display
    if top_features is not None and top_features < len(feature_names):
        top_indices = np.argsort(feature_importance)[-top_features:]
        selected_features = [feature_names[i] for i in top_indices]
    else:
        selected_features = feature_names
    
    # Prepare data for plotting
    plot_data = []
    
    for i, feature in enumerate(feature_names):
        if feature in selected_features:
            display_name = feature_display_names.get(feature, feature)
            
            # Group by ecoregion
            df_feature = pd.DataFrame({
                'ecoregion': ecoregions,
                'shap_value': shap_values[:, i]
            })
            
            # Calculate statistics per ecoregion
            stats = df_feature.groupby('ecoregion').agg({
                'shap_value': ['mean', 'std', 'count']
            }).round(4)
            
            stats.columns = ['mean', 'std', 'count']
            stats = stats.reset_index()
            
            # Add feature information
            stats['feature'] = feature
            stats['display_name'] = display_name
            stats['importance'] = feature_importance[i]
            
            # Filter ecoregions with sufficient samples
            stats = stats[stats['count'] >= 10]
            
            if len(stats) > 0:
                plot_data.append(stats)
    
    if not plot_data:
        print("✗ No data available for plotting")
        return None
    
    # Combine all data
    combined_data = pd.concat(plot_data, ignore_index=True)
    
    # Sort ecoregions
    all_ecoregions = sorted(combined_data['ecoregion'].unique())
    
    # Create figure
    fig, ax = plt.subplots(figsize=figsize)
    
    # Color palette for features
    unique_features = combined_data['display_name'].unique()
    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_features)))
    color_map = dict(zip(unique_features, colors))
    
    # Plot each feature
    for display_name in unique_features:
        feature_data = combined_data[combined_data['display_name'] == display_name]
        
        # Sort by ecoregion
        feature_data = feature_data.sort_values('ecoregion')
        
        # Get values in ecoregion order
        mean_values = []
        std_values = []
        eco_order = []
        
        for eco in all_ecoregions:
            eco_data = feature_data[feature_data['ecoregion'] == eco]
            if len(eco_data) > 0:
                mean_values.append(eco_data['mean'].iloc[0])
                std_values.append(eco_data['std'].iloc[0])
                eco_order.append(eco)
        
        if len(mean_values) > 1:
            # Plot line with error bars
            x_positions = [all_ecoregions.index(eco) for eco in eco_order]
            
            ax.errorbar(x_positions, mean_values, yerr=std_values,
                       color=color_map[display_name], linewidth=2,
                       marker='o', markersize=8, capsize=5,
                       label=display_name, alpha=0.8)
    
    # Format plot
    ax.set_xlabel('Ecoregion', fontsize=16, fontweight='bold')
    ax.set_ylabel('Mean SHAP Value', fontsize=16, fontweight='bold')
    
    # Set x-ticks to ecoregion names
    ax.set_xticks(range(len(all_ecoregions)))
    ax.set_xticklabels(all_ecoregions, rotation=45, ha='right', fontsize=12)
    
    # Add grid
    ax.grid(True, alpha=0.3)
    
    # Add legend
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', 
             fontsize=12, framealpha=0.7)
    
    # Add zero line
    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
    
    # Add title
    title = 'Feature Contributions Across Ecoregions'
    if top_features is not None:
        title += f' (Top {top_features} Features)'
    ax.set_title(title, fontsize=18, fontweight='bold', pad=20)
    
    plt.tight_layout()
    
    # Save figure if path provided
    if save_path:
        fig.savefig(f"{save_path}_ecoregion_contributions.png", 
                   dpi=300, bbox_inches='tight')
        fig.savefig(f"{save_path}_ecoregion_contributions.pdf", 
                   dpi=300, bbox_inches='tight')
        print(f"✓ Ecoregion contributions plot saved: {save_path}_ecoregion_contributions.png")
    
    return fig, combined_data

# ============================================================================
# ANALYSIS FUNCTIONS
# ============================================================================

def analyze_feature_importance(shap_data):
    """
    Analyze and rank feature importance based on SHAP values.
    
    Parameters:
    -----------
    shap_data : dict
        Dictionary containing SHAP data
        
    Returns:
    --------
    importance_df : pandas.DataFrame
        DataFrame with feature importance rankings
    """
    shap_values = shap_data['shap_values']
    feature_names = shap_data['feature_names']
    
    # Calculate various importance metrics
    importance_metrics = []
    
    for i, feature in enumerate(feature_names):
        abs_values = np.abs(shap_values[:, i])
        
        metrics = {
            'feature': feature,
            'mean_abs_shap': np.mean(abs_values),
            'std_abs_shap': np.std(abs_values),
            'mean_shap': np.mean(shap_values[:, i]),
            'std_shap': np.std(shap_values[:, i]),
            'percent_positive': np.mean(shap_values[:, i] > 0) * 100,
            'percent_negative': np.mean(shap_values[:, i] < 0) * 100
        }
        
        importance_metrics.append(metrics)
    
    importance_df = pd.DataFrame(importance_metrics)
    importance_df = importance_df.sort_values('mean_abs_shap', ascending=False)
    
    print("\n" + "=" * 60)
    print("FEATURE IMPORTANCE RANKING (based on mean |SHAP|)")
    print("=" * 60)
    for idx, row in importance_df.iterrows():
        sign = "+" if row['mean_shap'] > 0 else "-"
        print(f"{idx+1:2d}. {row['feature']:25s} "
              f"|SHAP|: {row['mean_abs_shap']:.4f} "
              f"(mean: {row['mean_shap']:+.4f}, "
              f"+%: {row['percent_positive']:.1f}%)")
    
    return importance_df

# ============================================================================
# MAIN ANALYSIS
# ============================================================================

def main():
    """
    Main function to run SHAP explainability analysis.
    """
    print("=" * 60)
    print("SHAP EXPLAINABILITY ANALYSIS")
    print("Corresponding to Fig. 7(a,b) in manuscript")
    print("=" * 60)
    
    # Load SHAP data
    shap_file_path = 'sample_data/shap_data.pkl'  # Update with your path
    shap_data = load_shap_data(shap_file_path)
    
    # Create simulated data if file not found
    if shap_data is None:
        print("\nCreating simulated SHAP data for demonstration...")
        shap_data = create_simulated_shap_data(n_samples=1000, n_features=14)
    
    # Create output directory
    output_dir = 'outputs/shap_analysis'
    os.makedirs(output_dir, exist_ok=True)
    
    # Plot A: Global feature importance (beeswarm plot)
    print("\nGenerating global feature importance plot (beeswarm)...")
    fig_a = plot_shap_beeswarm(
        shap_data, 
        figsize=(16, 10),
        save_path=f"{output_dir}/figure7a"
    )
    
    # Plot B: Ecoregion-specific feature contributions
    print("\nGenerating ecoregion-specific feature contributions plot...")
    fig_b, contributions_data = plot_ecoregion_shap_contributions(
        shap_data,
        figsize=(20, 12),
        top_features=10,  # Show top 10 features
        save_path=f"{output_dir}/figure7b"
    )
    
    # Analyze feature importance
    importance_df = analyze_feature_importance(shap_data)
    
    # Save detailed results
    importance_path = f"{output_dir}/feature_importance.csv"
    importance_df.to_csv(importance_path, index=False)
    
    if contributions_data is not None:
        contributions_path = f"{output_dir}/ecoregion_contributions.csv"
        contributions_data.to_csv(contributions_path, index=False)
    
    print(f"\n✓ Detailed results saved to {output_dir}/")
    
    # Print key findings
    print("\n" + "=" * 60)
    print("KEY FINDINGS FROM SHAP ANALYSIS")
    print("=" * 60)
    
    # Most important features
    top_features = importance_df.head(5)
    print("\nTop 5 most important features:")
    for idx, row in top_features.iterrows():
        effect = "positive" if row['mean_shap'] > 0 else "negative"
        print(f"  {row['feature']}: "
              f"importance={row['mean_abs_shap']:.4f} ({effect} effect)")
    
    # Feature interaction patterns
    print("\nFeature effect directions:")
    positive_features = importance_df[importance_df['mean_shap'] > 0]
    negative_features = importance_df[importance_df['mean_shap'] < 0]
    
    print(f"  Features with positive effect: {len(positive_features)}")
    print(f"  Features with negative effect: {len(negative_features)}")
    
    if len(positive_features) > 0:
        print("  Top positive features:")
        for feature in positive_features['feature'].head(3):
            print(f"    - {feature}")
    
    if len(negative_features) > 0:
        print("  Top negative features:")
        for feature in negative_features['feature'].head(3):
            print(f"    - {feature}")
    
    # Ecoregion variability
    if contributions_data is not None:
        variability = contributions_data.groupby('display_name')['std'].mean()
        most_variable = variability.idxmax()
        least_variable = variability.idxmin()
        
        print(f"\nEcoregion variability:")
        print(f"  Most variable feature across ecoregions: {most_variable}")
        print(f"  Least variable feature across ecoregions: {least_variable}")
    
    print("\n✓ SHAP analysis complete!")
    print(f"  Outputs saved to: {output_dir}/")
    print("=" * 60)

# ============================================================================
# EXECUTION
# ============================================================================

if __name__ == "__main__":
    main()
