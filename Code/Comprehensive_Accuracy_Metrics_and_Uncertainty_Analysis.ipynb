{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuONoImcQf85"
      },
      "outputs": [],
      "source": [
        "# ==================== Section 4: Comprehensive Accuracy Metrics and Uncertainty Analysis ====================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (confusion_matrix, classification_report,\n",
        "                           accuracy_score, f1_score, precision_score, recall_score,\n",
        "                           roc_auc_score, roc_curve, precision_recall_curve,\n",
        "                           average_precision_score, cohen_kappa_score, matthews_corrcoef)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Comprehensive Accuracy Metrics and Uncertainty Analysis\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load real data from the trained models (assuming you have the trainer object from previous section)\n",
        "# If you don't have the trainer object, you'll need to load your data differently\n",
        "print(\"Loading real data from trained models...\")\n",
        "\n",
        "# ==================== Load Real Data ====================\n",
        "# Assuming you have the trainer object from Section 5\n",
        "# If not, you'll need to modify this part to load your own data\n",
        "\n",
        "try:\n",
        "    # Try to use trainer object from previous section\n",
        "    from main import trainer  # Adjust import as needed\n",
        "\n",
        "    # Collect all test results from all trained ecoregions\n",
        "    all_y_true = []\n",
        "    all_y_pred_proba = []\n",
        "    all_y_pred = []\n",
        "\n",
        "    for ecoregion_id, result in trainer.results.items():\n",
        "        if 'y_test' in result and 'y_pred_proba' in result:\n",
        "            all_y_true.extend(result['y_test'])\n",
        "            all_y_pred_proba.extend(result['y_pred_proba'])\n",
        "            all_y_pred.extend(result['y_pred'])\n",
        "\n",
        "    y_true = np.array(all_y_true)\n",
        "    y_pred_proba = np.array(all_y_pred_proba)\n",
        "    y_pred = np.array(all_y_pred)\n",
        "\n",
        "    print(f\"Loaded real data: {len(y_true)} samples\")\n",
        "    print(f\"Class distribution: Stable={np.sum(y_true==0)}, Disturbance={np.sum(y_true==1)}\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Warning: Could not import trainer object from previous section.\")\n",
        "    print(\"Loading data from CSV file instead...\")\n",
        "\n",
        "    # Alternative: Load from CSV if you have saved predictions\n",
        "    try:\n",
        "        # Modify this path to your actual data file\n",
        "        data_path = '/content/drive/MyDrive/predictions.csv'\n",
        "        df = pd.read_csv(data_path)\n",
        "\n",
        "        # Assuming columns: 'y_true', 'y_pred_proba', 'y_pred'\n",
        "        y_true = df['y_true'].values\n",
        "        y_pred_proba = df['y_pred_proba'].values\n",
        "        y_pred = df['y_pred'].values\n",
        "\n",
        "        print(f\"Loaded data from CSV: {len(y_true)} samples\")\n",
        "        print(f\"Class distribution: Stable={np.sum(y_true==0)}, Disturbance={np.sum(y_true==1)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        print(\"Using sample data for demonstration...\")\n",
        "\n",
        "        # Fallback to sample data if real data not available\n",
        "        np.random.seed(42)\n",
        "        n_samples = 1000\n",
        "\n",
        "        # Generate realistic sample data\n",
        "        y_true = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
        "        y_pred_proba = np.zeros(n_samples)\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            if y_true[i] == 0:\n",
        "                y_pred_proba[i] = np.random.beta(2, 8)\n",
        "            else:\n",
        "                y_pred_proba[i] = np.random.beta(8, 2)\n",
        "\n",
        "        y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "        print(f\"Using sample data: {n_samples} samples\")\n",
        "\n",
        "# Apply different thresholds\n",
        "thresholds = [0.3, 0.5, 0.7]\n",
        "results = {}\n",
        "\n",
        "for threshold in thresholds:\n",
        "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "    # Calculate comprehensive metrics\n",
        "    cm = confusion_matrix(y_true, y_pred_thresh)\n",
        "\n",
        "    # Handle cases where confusion matrix might not be 2x2\n",
        "    if cm.shape == (2, 2):\n",
        "        TN, FP, FN, TP = cm.ravel()\n",
        "    else:\n",
        "        # If only one class present, handle accordingly\n",
        "        TN = cm[0, 0] if cm.shape[0] > 0 else 0\n",
        "        FP = cm[0, 1] if cm.shape[1] > 1 else 0\n",
        "        FN = cm[1, 0] if cm.shape[0] > 1 else 0\n",
        "        TP = cm[1, 1] if cm.shape[0] > 1 and cm.shape[1] > 1 else 0\n",
        "\n",
        "    # Basic metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred_thresh)\n",
        "    precision = precision_score(y_true, y_pred_thresh, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred_thresh, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred_thresh, zero_division=0)\n",
        "    kappa = cohen_kappa_score(y_true, y_pred_thresh)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred_thresh)\n",
        "\n",
        "    # Producer's Accuracy = 1 - Omission Error\n",
        "    pa_0 = TN / (TN + FP) if (TN + FP) > 0 else 0  # Stable class\n",
        "    pa_1 = TP / (TP + FN) if (TP + FN) > 0 else 0  # Disturbance class\n",
        "\n",
        "    # User's Accuracy = 1 - Commission Error\n",
        "    ua_0 = TN / (TN + FN) if (TN + FN) > 0 else 0  # Stable class\n",
        "    ua_1 = TP / (TP + FP) if (TP + FP) > 0 else 0  # Disturbance class\n",
        "\n",
        "    # Omission Error\n",
        "    oe_0 = 1 - pa_0  # Stable class\n",
        "    oe_1 = 1 - pa_1  # Disturbance class\n",
        "\n",
        "    # Commission Error\n",
        "    ce_0 = 1 - ua_0  # Stable class\n",
        "    ce_1 = 1 - ua_1  # Disturbance class\n",
        "\n",
        "    # Overall accuracy\n",
        "    mean_pa = (pa_0 + pa_1) / 2\n",
        "    mean_ua = (ua_0 + ua_1) / 2\n",
        "\n",
        "    results[threshold] = {\n",
        "        'threshold': threshold,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'kappa': kappa,\n",
        "        'mcc': mcc,\n",
        "        'pa_0': pa_0,\n",
        "        'pa_1': pa_1,\n",
        "        'ua_0': ua_0,\n",
        "        'ua_1': ua_1,\n",
        "        'oe_0': oe_0,\n",
        "        'oe_1': oe_1,\n",
        "        'ce_0': ce_0,\n",
        "        'ce_1': ce_1,\n",
        "        'mean_pa': mean_pa,\n",
        "        'mean_ua': mean_ua,\n",
        "        'cm': cm,\n",
        "        'y_pred': y_pred_thresh\n",
        "    }\n",
        "\n",
        "class ComprehensiveAccuracyAnalysis:\n",
        "    \"\"\"Comprehensive Accuracy Analysis\"\"\"\n",
        "\n",
        "    def __init__(self, y_true, y_pred_proba):\n",
        "        self.y_true = y_true\n",
        "        self.y_pred_proba = y_pred_proba\n",
        "        self.results = {}\n",
        "\n",
        "    def analyze_thresholds(self, thresholds=None):\n",
        "        \"\"\"Analyze performance under different thresholds\"\"\"\n",
        "        if thresholds is None:\n",
        "            thresholds = np.arange(0.1, 1.0, 0.05)\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            y_pred = (self.y_pred_proba >= threshold).astype(int)\n",
        "            self.results[threshold] = self.calculate_all_metrics(self.y_true, y_pred, self.y_pred_proba)\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def calculate_all_metrics(self, y_true, y_pred, y_pred_proba):\n",
        "        \"\"\"Calculate all metrics\"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        # Basic metrics\n",
        "        metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "        metrics['precision'] = precision_score(y_true, y_pred, zero_division=0)\n",
        "        metrics['recall'] = recall_score(y_true, y_pred, zero_division=0)\n",
        "        metrics['f1'] = f1_score(y_true, y_pred, zero_division=0)\n",
        "        metrics['kappa'] = cohen_kappa_score(y_true, y_pred)\n",
        "        metrics['mcc'] = matthews_corrcoef(y_true, y_pred)\n",
        "\n",
        "        # AUC metrics\n",
        "        if len(np.unique(y_true)) > 1:\n",
        "            metrics['auc_roc'] = roc_auc_score(y_true, y_pred_proba)\n",
        "            metrics['average_precision'] = average_precision_score(y_true, y_pred_proba)\n",
        "        else:\n",
        "            metrics['auc_roc'] = 0.5\n",
        "            metrics['average_precision'] = 0.5\n",
        "\n",
        "        # Confusion matrix related metrics\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        metrics['cm'] = cm\n",
        "\n",
        "        if cm.shape == (2, 2):\n",
        "            TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "            # Producer's Accuracy\n",
        "            metrics['pa_0'] = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "            metrics['pa_1'] = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "            # User's Accuracy\n",
        "            metrics['ua_0'] = TN / (TN + FN) if (TN + FN) > 0 else 0\n",
        "            metrics['ua_1'] = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "\n",
        "            # Omission Error\n",
        "            metrics['oe_0'] = 1 - metrics['pa_0']\n",
        "            metrics['oe_1'] = 1 - metrics['pa_1']\n",
        "\n",
        "            # Commission Error\n",
        "            metrics['ce_0'] = 1 - metrics['ua_0']\n",
        "            metrics['ce_1'] = 1 - metrics['ua_1']\n",
        "\n",
        "            # Overall\n",
        "            metrics['mean_pa'] = (metrics['pa_0'] + metrics['pa_1']) / 2\n",
        "            metrics['mean_ua'] = (metrics['ua_0'] + metrics['ua_1']) / 2\n",
        "\n",
        "            # Balanced accuracy\n",
        "            metrics['balanced_accuracy'] = (metrics['pa_0'] + metrics['pa_1']) / 2\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_comprehensive_analysis(self, optimal_threshold=0.5):\n",
        "        \"\"\"Plot comprehensive analysis visualizations\"\"\"\n",
        "        # Use results with optimal threshold\n",
        "        y_pred_opt = (self.y_pred_proba >= optimal_threshold).astype(int)\n",
        "        metrics_opt = self.calculate_all_metrics(self.y_true, y_pred_opt, self.y_pred_proba)\n",
        "\n",
        "        fig = plt.figure(figsize=(18, 12))\n",
        "\n",
        "        # Create subplot grid\n",
        "        gs = fig.add_gridspec(3, 3)\n",
        "\n",
        "        # 1. Confusion Matrix\n",
        "        ax1 = fig.add_subplot(gs[0, 0])\n",
        "        cm = metrics_opt['cm']\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
        "                   xticklabels=['Stable', 'Disturbance'], yticklabels=['Stable', 'Disturbance'])\n",
        "        ax1.set_title(f'Confusion Matrix (Threshold={optimal_threshold})', fontsize=12, fontweight='bold')\n",
        "        ax1.set_xlabel('Predicted Label')\n",
        "        ax1.set_ylabel('True Label')\n",
        "\n",
        "        # Add accuracy metrics text\n",
        "        cm_text = f\"\"\"\n",
        "Producer's Accuracy (PA):\n",
        "Stable: {metrics_opt.get('pa_0', 0):.3f}\n",
        "Disturbance: {metrics_opt.get('pa_1', 0):.3f}\n",
        "\n",
        "User's Accuracy (UA):\n",
        "Stable: {metrics_opt.get('ua_0', 0):.3f}\n",
        "Disturbance: {metrics_opt.get('ua_1', 0):.3f}\n",
        "\n",
        "Omission Error (OE):\n",
        "Stable: {metrics_opt.get('oe_0', 0):.3f}\n",
        "Disturbance: {metrics_opt.get('oe_1', 0):.3f}\n",
        "\n",
        "Commission Error (CE):\n",
        "Stable: {metrics_opt.get('ce_0', 0):.3f}\n",
        "Disturbance: {metrics_opt.get('ce_1', 0):.3f}\n",
        "\"\"\"\n",
        "        ax1.text(2.8, 0.5, cm_text, fontsize=9, verticalalignment='center',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "        # 2. ROC Curve\n",
        "        ax2 = fig.add_subplot(gs[0, 1])\n",
        "        fpr, tpr, _ = roc_curve(self.y_true, self.y_pred_proba)\n",
        "        ax2.plot(fpr, tpr, label=f'ROC Curve (AUC={metrics_opt.get(\"auc_roc\", 0):.3f})', linewidth=2)\n",
        "        ax2.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
        "        ax2.set_xlabel('False Positive Rate (FPR)')\n",
        "        ax2.set_ylabel('True Positive Rate (TPR)')\n",
        "        ax2.set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        # 3. PR Curve\n",
        "        ax3 = fig.add_subplot(gs[0, 2])\n",
        "        precision_curve, recall_curve, _ = precision_recall_curve(self.y_true, self.y_pred_proba)\n",
        "        ax3.plot(recall_curve, precision_curve,\n",
        "                label=f'PR Curve (AP={metrics_opt.get(\"average_precision\", 0):.3f})',\n",
        "                linewidth=2)\n",
        "        ax3.set_xlabel('Recall')\n",
        "        ax3.set_ylabel('Precision')\n",
        "        ax3.set_title('Precision-Recall Curve', fontsize=12, fontweight='bold')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "\n",
        "        # 4. Threshold Analysis\n",
        "        ax4 = fig.add_subplot(gs[1, 0])\n",
        "        thresholds = np.arange(0.1, 1.0, 0.05)\n",
        "        accuracies = []\n",
        "        f1_scores = []\n",
        "\n",
        "        for thresh in thresholds:\n",
        "            y_pred = (self.y_pred_proba >= thresh).astype(int)\n",
        "            accuracies.append(accuracy_score(self.y_true, y_pred))\n",
        "            f1_scores.append(f1_score(self.y_true, y_pred, zero_division=0))\n",
        "\n",
        "        ax4.plot(thresholds, accuracies, 'b-', label='Accuracy', linewidth=2)\n",
        "        ax4.plot(thresholds, f1_scores, 'r-', label='F1 Score', linewidth=2)\n",
        "        ax4.axvline(x=optimal_threshold, color='g', linestyle='--',\n",
        "                   label=f'Optimal Threshold={optimal_threshold}')\n",
        "        ax4.set_xlabel('Threshold')\n",
        "        ax4.set_ylabel('Score')\n",
        "        ax4.set_title('Threshold Sensitivity Analysis', fontsize=12, fontweight='bold')\n",
        "        ax4.legend()\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "\n",
        "        # 5. Prediction Probability Distribution\n",
        "        ax5 = fig.add_subplot(gs[1, 1])\n",
        "        ax5.hist(self.y_pred_proba[self.y_true==0], bins=30, alpha=0.7,\n",
        "                label='Stable (True)', color='blue', density=True)\n",
        "        ax5.hist(self.y_pred_proba[self.y_true==1], bins=30, alpha=0.7,\n",
        "                label='Disturbance (True)', color='red', density=True)\n",
        "        ax5.axvline(x=optimal_threshold, color='g', linestyle='--',\n",
        "                   label=f'Threshold={optimal_threshold}')\n",
        "        ax5.set_xlabel('Predicted Probability')\n",
        "        ax5.set_ylabel('Density')\n",
        "        ax5.set_title('Prediction Probability Distribution', fontsize=12, fontweight='bold')\n",
        "        ax5.legend()\n",
        "\n",
        "        # 6. Radar Chart of Accuracy Metrics\n",
        "        ax6 = fig.add_subplot(gs[1, 2], polar=True)\n",
        "        metrics_names = ['Accuracy', 'F1', 'Precision', 'Recall', 'Kappa', 'MCC']\n",
        "        metrics_values = [metrics_opt.get('accuracy', 0), metrics_opt.get('f1', 0),\n",
        "                         metrics_opt.get('precision', 0), metrics_opt.get('recall', 0),\n",
        "                         metrics_opt.get('kappa', 0), metrics_opt.get('mcc', 0)]\n",
        "\n",
        "        angles = np.linspace(0, 2*np.pi, len(metrics_names), endpoint=False).tolist()\n",
        "        metrics_values = metrics_values + [metrics_values[0]]\n",
        "        angles = angles + [angles[0]]\n",
        "\n",
        "        ax6.plot(angles, metrics_values, 'o-', linewidth=2)\n",
        "        ax6.fill(angles, metrics_values, alpha=0.25)\n",
        "        ax6.set_thetagrids(np.degrees(angles[:-1]), metrics_names)\n",
        "        ax6.set_ylim(0, 1)\n",
        "        ax6.set_title('Accuracy Metrics Radar Chart', fontsize=12, fontweight='bold', y=1.1)\n",
        "\n",
        "        # 7. Uncertainty Analysis\n",
        "        ax7 = fig.add_subplot(gs[2, :])\n",
        "        self.plot_uncertainty_analysis(ax7, optimal_threshold)\n",
        "\n",
        "        plt.suptitle('Comprehensive Accuracy Metrics and Uncertainty Analysis',\n",
        "                    fontsize=16, fontweight='bold', y=0.98)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def plot_uncertainty_analysis(self, ax, threshold):\n",
        "        \"\"\"Plot uncertainty analysis\"\"\"\n",
        "        # Calculate uncertainty metrics\n",
        "        uncertainty_zones = {\n",
        "            'High Confidence Correct': 0,\n",
        "            'High Confidence Error': 0,\n",
        "            'Low Confidence': 0\n",
        "        }\n",
        "\n",
        "        confidence = np.abs(self.y_pred_proba - 0.5) * 2  # Convert to 0-1 confidence\n",
        "\n",
        "        for i in range(len(self.y_true)):\n",
        "            pred_class = 1 if self.y_pred_proba[i] >= threshold else 0\n",
        "            is_correct = pred_class == self.y_true[i]\n",
        "\n",
        "            if confidence[i] >= 0.6:  # High confidence\n",
        "                if is_correct:\n",
        "                    uncertainty_zones['High Confidence Correct'] += 1\n",
        "                else:\n",
        "                    uncertainty_zones['High Confidence Error'] += 1\n",
        "            else:  # Low confidence\n",
        "                uncertainty_zones['Low Confidence'] += 1\n",
        "\n",
        "        total = sum(uncertainty_zones.values())\n",
        "\n",
        "        # Plot pie chart\n",
        "        labels = list(uncertainty_zones.keys())\n",
        "        sizes = list(uncertainty_zones.values())\n",
        "        colors = ['lightgreen', 'lightcoral', 'lightblue']\n",
        "        explode = (0.1, 0.1, 0.1)\n",
        "\n",
        "        wedges, texts, autotexts = ax.pie(sizes, explode=explode, labels=labels,\n",
        "                                         colors=colors, autopct='%1.1f%%',\n",
        "                                         shadow=True, startangle=90)\n",
        "\n",
        "        for autotext in autotexts:\n",
        "            autotext.set_color('black')\n",
        "            autotext.set_fontweight('bold')\n",
        "\n",
        "        ax.set_title('Prediction Uncertainty Analysis', fontsize=12, fontweight='bold')\n",
        "\n",
        "        # Add uncertainty sources text\n",
        "        uncertainty_text = f\"\"\"\n",
        "Uncertainty Source Analysis:\n",
        "\n",
        "1. Training Sample Uncertainty:\n",
        "   • Some samples from Hansen data have positional and classification errors\n",
        "   • Uneven sample distribution across ecoregions\n",
        "\n",
        "2. Image Data Uncertainty:\n",
        "   • Cloud and snow contamination in annual composite images\n",
        "   • Sensor differences and temporal inconsistencies\n",
        "\n",
        "3. Model Uncertainty:\n",
        "   • Sensitivity to threshold selection\n",
        "   • Feature differences across ecoregions\n",
        "   • Time series discontinuity issues\n",
        "\n",
        "4. Application Uncertainty:\n",
        "   • Mixed pixel effects\n",
        "   • Progressive disturbance year attribution\n",
        "   • Small patch detection limitations\n",
        "\n",
        "Total Samples: {total}\n",
        "High Confidence Correct: {uncertainty_zones['High Confidence Correct']} ({uncertainty_zones['High Confidence Correct']/total*100:.1f}%)\n",
        "High Confidence Error: {uncertainty_zones['High Confidence Error']} ({uncertainty_zones['High Confidence Error']/total*100:.1f}%)\n",
        "Low Confidence Regions: {uncertainty_zones['Low Confidence']} ({uncertainty_zones['Low Confidence']/total*100:.1f}%)\n",
        "\"\"\"\n",
        "\n",
        "        # Add text to the right of pie chart\n",
        "        ax.text(1.8, 0, uncertainty_text, transform=ax.transAxes, fontsize=9,\n",
        "               verticalalignment='center',\n",
        "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "    def generate_report(self, optimal_threshold=0.5):\n",
        "        \"\"\"Generate analysis report\"\"\"\n",
        "        y_pred_opt = (self.y_pred_proba >= optimal_threshold).astype(int)\n",
        "        metrics_opt = self.calculate_all_metrics(self.y_true, y_pred_opt, self.y_pred_proba)\n",
        "\n",
        "        report = f\"\"\"\n",
        "Comprehensive Accuracy Analysis Report\n",
        "{'='*60}\n",
        "\n",
        "1. Overall Performance\n",
        "{'='*30}\n",
        "Overall Accuracy (OA): {metrics_opt.get('accuracy', 0):.4f}\n",
        "F1 Score: {metrics_opt.get('f1', 0):.4f}\n",
        "Kappa Coefficient: {metrics_opt.get('kappa', 0):.4f}\n",
        "Matthews Correlation Coefficient: {metrics_opt.get('mcc', 0):.4f}\n",
        "AUC-ROC: {metrics_opt.get('auc_roc', 0):.4f}\n",
        "Average Precision (AP): {metrics_opt.get('average_precision', 0):.4f}\n",
        "\n",
        "2. Classification Accuracy Breakdown\n",
        "{'='*30}\n",
        "2.1 Producer's Accuracy (Complement of Omission Error):\n",
        "   • Stable class: {metrics_opt.get('pa_0', 0):.4f} (Omission Error: {metrics_opt.get('oe_0', 0):.4f})\n",
        "   • Disturbance class: {metrics_opt.get('pa_1', 0):.4f} (Omission Error: {metrics_opt.get('oe_1', 0):.4f})\n",
        "   • Mean Producer's Accuracy: {metrics_opt.get('mean_pa', 0):.4f}\n",
        "\n",
        "2.2 User's Accuracy (Complement of Commission Error):\n",
        "   • Stable class: {metrics_opt.get('ua_0', 0):.4f} (Commission Error: {metrics_opt.get('ce_0', 0):.4f})\n",
        "   • Disturbance class: {metrics_opt.get('ua_1', 0):.4f} (Commission Error: {metrics_opt.get('ce_1', 0):.4f})\n",
        "   • Mean User's Accuracy: {metrics_opt.get('mean_ua', 0):.4f}\n",
        "\n",
        "3. Threshold Sensitivity Analysis\n",
        "{'='*30}\n",
        "Optimal Threshold: {optimal_threshold}\n",
        "F1 Score Variation with Threshold: ±{np.std([v.get('f1', 0) for v in results.values()]):.4f}\n",
        "\n",
        "4. Uncertainty Assessment\n",
        "{'='*30}\n",
        "4.1 Data Uncertainty:\n",
        "   • Training sample positional errors\n",
        "   • Image data quality issues\n",
        "   • Ecoregion feature differences\n",
        "\n",
        "4.2 Model Uncertainty:\n",
        "   • Threshold selection sensitivity\n",
        "   • Time series analysis uncertainty\n",
        "   • Small patch detection limitations\n",
        "\n",
        "4.3 Application Uncertainty:\n",
        "   • Progressive disturbance year attribution\n",
        "   • Mixed pixel effects\n",
        "   • Spatial scale conversion issues\n",
        "\n",
        "5. Recommendations\n",
        "{'='*30}\n",
        "1. Report both Producer's and User's Accuracy, not just overall accuracy\n",
        "2. Provide confusion matrices as supplementary material\n",
        "3. Discuss the impact of threshold selection on results\n",
        "4. Analyze main sources of uncertainty\n",
        "5. Consider providing uncertainty maps\n",
        "\n",
        "{'='*60}\n",
        "Report Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "Total Samples Analyzed: {len(self.y_true)}\n",
        "\"\"\"\n",
        "\n",
        "        return report\n",
        "\n",
        "# Execute comprehensive analysis\n",
        "print(\"\\nExecuting comprehensive accuracy analysis...\")\n",
        "analyzer = ComprehensiveAccuracyAnalysis(y_true, y_pred_proba)\n",
        "\n",
        "# Analyze different thresholds\n",
        "thresholds = [0.3, 0.5, 0.7]\n",
        "for threshold in thresholds:\n",
        "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
        "    metrics = analyzer.calculate_all_metrics(y_true, y_pred_thresh, y_pred_proba)\n",
        "    results[threshold] = metrics\n",
        "\n",
        "# Plot comprehensive analysis visualizations\n",
        "print(\"\\nGenerating analysis visualizations...\")\n",
        "fig = analyzer.plot_comprehensive_analysis(optimal_threshold=0.5)\n",
        "\n",
        "# Generate report\n",
        "print(\"\\nGenerating analysis report...\")\n",
        "report = analyzer.generate_report(optimal_threshold=0.5)\n",
        "print(report)\n",
        "\n",
        "# Save results\n",
        "print(\"\\nSaving analysis results...\")\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "os.makedirs('analysis_results', exist_ok=True)\n",
        "\n",
        "# Save metrics results\n",
        "metrics_summary = {}\n",
        "for threshold, metrics in results.items():\n",
        "    # Only save serializable data\n",
        "    save_metrics = {k: v for k, v in metrics.items() if k != 'cm'}\n",
        "    if 'cm' in metrics:\n",
        "        save_metrics['cm'] = metrics['cm'].tolist()\n",
        "    metrics_summary[str(threshold)] = save_metrics\n",
        "\n",
        "with open('analysis_results/comprehensive_metrics.json', 'w') as f:\n",
        "    json.dump(metrics_summary, f, indent=2)\n",
        "\n",
        "# Save report\n",
        "with open('analysis_results/accuracy_analysis_report.txt', 'w') as f:\n",
        "    f.write(report)\n",
        "\n",
        "# Save figure\n",
        "fig.savefig('analysis_results/comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "print(\"✓ Analysis results saved:\")\n",
        "print(\"  - analysis_results/comprehensive_metrics.json\")\n",
        "print(\"  - analysis_results/accuracy_analysis_report.txt\")\n",
        "print(\"  - analysis_results/comprehensive_analysis.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Manuscript Revision Suggestions\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\"\"\n",
        "For addressing reviewer comments on accuracy metrics:\n",
        "\n",
        "1. Report in Results Section:\n",
        "   • Producer's Accuracy and User's Accuracy\n",
        "   • Omission Error and Commission Error\n",
        "   • Balanced Accuracy\n",
        "\n",
        "2. Specify in Methods Section:\n",
        "   • Threshold used and justification for threshold selection\n",
        "   • Calculation methods for accuracy metrics\n",
        "\n",
        "3. Analyze in Discussion Section:\n",
        "   • Main sources of error\n",
        "   • Impact of uncertainty on results\n",
        "   • Sensitivity to threshold selection\n",
        "\n",
        "4. Provide in Supplementary Materials:\n",
        "   • Complete confusion matrices\n",
        "   • Detailed accuracy metrics for each ecoregion\n",
        "   • Uncertainty analysis results\n",
        "   • Threshold sensitivity analysis\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Comprehensive Accuracy Analysis Completed!\")\n",
        "print(\"=\"*60)"
      ]
    }
  ]
}